from typing import List, Annotated, Optional
from typing_extensions import TypedDict
import operator
import httpx
import os
from langgraph.graph import StateGraph, END

from dotenv import load_dotenv

load_dotenv()

class AgentState(TypedDict):
    initial_user_input: str # The initial information/topic from the user
    casey_question: str     # The question generated by Curious Casey
    finn_explanation: str   # The explanation provided by Factual Finn
    current_turn: int       # To track turns
    conversation_history: Annotated[List[str], operator.add] # operator.add will append to this list
    generated_audio_file: Optional[str] # New optional field for the audio file path/name

BACKEND_API_BASE_URL = os.getenv("BACKEND_API_URL", "http://127.0.0.1:8000")

async def invoke_curious_casey(state: AgentState) -> dict:
    """
    Node function for Curious Casey.
    Takes the initial user input from the state, calls the /ask_follow_up API,
    and returns Casey's question to update the state.
    """
    current_turn = state.get("current_turn", 0) # Get current turn, default to 0
    conversation_history = state.get("conversation_history", [])

    prompt_input_for_casey = ""

    if current_turn == 0: # First turn for Casey
        prompt_input_for_casey = state.get("initial_user_input")
        if not prompt_input_for_casey:
            print("Error: Initial user input not found for Casey's first turn.")
            return {"casey_question": "Error: No initial topic provided.", "conversation_history": ["Error: No initial topic."]}
        print(f"Curious Casey (Turn {current_turn}) received initial input: '{prompt_input_for_casey[:100]}...'")
        # For the very first question, Casey's fine-tuning expects "The knowledgeable AI just said: '[initial_input]'"
        # So, we frame the initial user input as if it was a statement.
        # This aligns with how we tested Casey in Ollama directly.
        api_payload_statement = prompt_input_for_casey
    else: # Subsequent turns for Casey
        last_finn_explanation = state.get("finn_explanation")
        if not last_finn_explanation:
            print("Error: Factual Finn's last explanation not found for Casey's follow-up.")
            return {"casey_question": "Error: Missing context from Factual Finn.", "conversation_history": ["Error: Missing context for Casey."]}

        # For follow-up questions, Casey's fine-tuning expects the previous statement.
        # We can prepend some history for more context if desired, but for now,
        # let's stick to the fine-tuning format: "The knowledgeable AI just said: [Finn's last explanation]"
        # More advanced context handling can be added later.
        prompt_input_for_casey = last_finn_explanation
        print(f"Curious Casey (Turn {current_turn}) received Finn's last explanation: '{prompt_input_for_casey[:100]}...'")
        api_payload_statement = prompt_input_for_casey

    # Prepare the request for the /ask_follow_up endpoint
    # The payload for /ask_follow_up expects "previous_statement"
    payload = {"previous_statement": api_payload_statement}
    api_url = f"{BACKEND_API_BASE_URL}/ask_follow_up"

    casey_question_text = ""
    new_history_entries = []

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(api_url, json=payload, timeout=180.0)
            response.raise_for_status()
            response_data = response.json()
            casey_question_text = response_data.get("follow_up_question")

            if not casey_question_text:
                print("Error: 'follow_up_question' not found in API response or was empty.")
                casey_question_text = "I'm not sure what to ask next."
            else:
                print(f"Curious Casey generated question: '{casey_question_text}'")

        if current_turn == 0: # Only add initial topic on the first turn history by Casey
            new_history_entries.append(f"Initial Topic: {state.get('initial_user_input')}")
        new_history_entries.append(f"Curious Casey: {casey_question_text}")

    # ... (keep existing error handling as is) ...
    except httpx.HTTPStatusError as e:
        print(f"HTTP error calling /ask_follow_up for Curious Casey: {e.response.status_code} - {e.response.text}")
        casey_question_text = "Error: Could not get a question from Curious Casey due to an API error."
        new_history_entries.append(f"Error: API call to Curious Casey failed - {e.response.status_code}")
    except httpx.RequestError as e:
        print(f"Request error calling /ask_follow_up for Curious Casey: {e}")
        casey_question_text = "Error: Could not connect to the backend to get a question from Curious Casey."
        new_history_entries.append(f"Error: Network error calling Curious Casey - {e}")
    except Exception as e:
        print(f"An unexpected error occurred in invoke_curious_casey: {e}")
        import traceback
        traceback.print_exc()
        casey_question_text = "An unexpected error occurred while I was thinking of a question."
        new_history_entries.append(f"Error: Unexpected error in Curious Casey node - {e}")


    return {
        "casey_question": casey_question_text,
        "conversation_history": new_history_entries,
        "current_turn": current_turn + 1 # Increment turn count
    }
    
async def invoke_factual_finn(state: AgentState) -> dict:
    """
    Node function for Factual Finn.
    Takes Casey's question and the initial user input from the state,
    calls the /explain API to get Finn's explanation,
    and returns Finn's explanation to update the state.
    """
    print("---INVOKING FACTUAL FINN NODE---")
    casey_question = state.get("casey_question")
    initial_input = state.get("initial_user_input")

    if not casey_question or not initial_input:
        print("Error: Casey's question or initial input not found in state for Factual Finn.")
        return {}

    print(f"Factual Finn received question: '{casey_question}' regarding: '{initial_input[:100]}...'")

    # Construct the instruction for Factual Finn to answer Casey's question
    # about the initial_input.
    # We want Finn to still explain the 'initial_input' but frame its explanation
    # in the context of Casey's question.
    # Option 1: Make Casey's question part of the 'input_text' for Finn's standard instruction.
    # This might be simpler for the existing /explain endpoint.

    # Let's try framing the instruction for Finn to explain the initial_input,
    # and its fine-tuning should make it address the most salient points,
    # hopefully aligning with what a good question from Casey would target.
    # A more advanced approach would be to modify the /explain endpoint or
    # create a new one that explicitly takes a "context" and a "question".
    # For now, let's have Finn explain the 'initial_input', and we assume
    # Casey's question has primed the context for what aspects are interesting.

    # The /explain endpoint expects "information_text" and an "instruction".
    # Factual Finn's default instruction is:
    # "Explain the following information in a direct and factual style, focusing on key points."
    # We will use the 'initial_user_input' as the 'information_text'.
    # Casey's question has set the stage. Finn will explain the initial_user_input.

    # An alternative and potentially better instruction for Finn here would be:
    # f"In response to the question: '{casey_question}', explain the relevant aspects of the following information in a direct and factual style, focusing on key points."
    # This makes Finn directly address Casey's question using the initial_input as context.
    # Let's use this more direct approach.

    finn_instruction = f"In response to the question: '{casey_question}', explain the relevant aspects of the following information in a direct and factual style, focusing on key points."

    payload = {
        "information_text": initial_input, # Finn explains the original topic
        "instruction": finn_instruction    # but guided by Casey's question
    }
    api_url = f"{BACKEND_API_BASE_URL}/explain"

    finn_explanation_text = ""
    new_history_entry = ""

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(api_url, json=payload, timeout=180.0) # Increased timeout
            response.raise_for_status()

            response_data = response.json()
            finn_explanation_text = response_data.get("explanation")

            if not finn_explanation_text:
                print("Error: 'explanation' not found in API response from Factual Finn or was empty.")
                finn_explanation_text = "Sorry, I could not provide an explanation at this time." # Fallback
            else:
                print(f"Factual Finn generated explanation: '{finn_explanation_text[:200]}...'")

        new_history_entry = f"Factual Finn: {finn_explanation_text}"

    except httpx.HTTPStatusError as e:
        error_detail = e.response.text
        print(f"HTTP error calling /explain for Factual Finn: {e.response.status_code} - {error_detail}")
        finn_explanation_text = f"Error: Could not get an explanation from Factual Finn due to an API error ({e.response.status_code})."
        new_history_entry = f"Error: API call to Factual Finn failed - {e.response.status_code}"
    except httpx.RequestError as e:
        print(f"Request error calling /explain for Factual Finn: {e}")
        finn_explanation_text = "Error: Could not connect to the backend to get an explanation from Factual Finn."
        new_history_entry = f"Error: Network error calling Factual Finn - {e}"
    except Exception as e:
        print(f"An unexpected error occurred in invoke_factual_finn: {e}")
        import traceback
        traceback.print_exc()
        finn_explanation_text = "An unexpected error occurred while I was preparing an explanation."
        new_history_entry = f"Error: Unexpected error in Factual Finn node - {e}"

    return {
        "finn_explanation": finn_explanation_text,
        "conversation_history": [new_history_entry] # LangGraph will append this
    }
    
MAX_CONVERSATION_TURNS = 3 # Define how many Q&A cycles (Casey asks, Finn answers)

def should_continue(state: AgentState) -> str:
    """
    Determines whether the conversation should continue or end.
    """
    print(f"---CHECKING SHOULD_CONTINUE (Current Turn: {state.get('current_turn', 0)})---")
    if state.get("current_turn", 0) >= MAX_CONVERSATION_TURNS:
        print("Max turns reached. Ending conversation.")
        return "END_CONVERSATION" # Route to END
    else:
        print("Continuing conversation.")
        return "CONTINUE_TO_CASEY" 
    
# --- Define the LangGraph Workflow ---
print("Defining LangGraph workflow...")
workflow = StateGraph(AgentState)

workflow.add_node("CURIOUS_CASEY", invoke_curious_casey)
workflow.add_node("FACTUAL_FINN", invoke_factual_finn)

workflow.set_entry_point("CURIOUS_CASEY")
workflow.add_edge("CURIOUS_CASEY", "FACTUAL_FINN")

# Add the conditional edge after FACTUAL_FINN
workflow.add_conditional_edges(
    "FACTUAL_FINN", # Source node
    should_continue, # Function to decide the next path
    {
        "CONTINUE_TO_CASEY": "CURIOUS_CASEY", # If should_continue returns "CONTINUE_TO_CASEY"
        "END_CONVERSATION": END              # If should_continue returns "END_CONVERSATION"
    }
)

app_graph = workflow.compile()
print("LangGraph workflow compiled successfully with multi-turn loop.")